---
title: "Assignment 2"
author: "Umer Salman Khan"
date: "3/20/2022"
output: html_document
---
Question 1:
(a)
```{r}
set.seed(4)
x=sort(rnorm(n=50,mean=0,sd=2)) #normal distribution for x values
x
y=rnorm(n = 50, mean = 2 + 1.5*x, sd = 10) #normal distribution for y values
y
reg<-lm(y ~ x,) #running the linear regression model
coeff = coefficients(reg) #coefficients of the model
coeff
plot(x, y) #plotting y against x
abline(reg, col = "darkgreen") #adding a fitted linear line to the plot
summary(reg) #summary of the regression model

```

(b)We received p values using the summary(reg) function. Since the p value of B1 is 0.0716 which is greater than the given 0.05 standard of significance, we can say that the results are not statistically significant and we do not reject the null hypothesis.

(c)
```{r}
set.seed(200)
x=sort(rnorm(n=50,mean=0,sd=2)) #normal distribution for x values
x
y=rnorm(n = 50, mean = 2 + 1.5*x, sd = 10) #normal distribution for y values
y
reg<-lm(y ~ x,) #running the linear regression model
coeff = coefficients(reg) #coefficients os the model
coeff
plot(x, y) #plotting y against x
abline(reg, col = "blue") #adding a fitted linear line to the plot
summary(reg) #summary of the regression model
```
Since the p value of B1 is 0.0544 which is greater than 0.05, we can say that the results are not statistically significant and we do not reject the null hypothesis.

(d) Hypothesis testing is used by doing statistical analysis on the data to either reject or accept your hypothesis. However, there are several limitations to it as mentioned in the article itself. As mentioned in the article,  ideally, statisticians say, researchers should set out to prove a specific hypothesis before a study begins. Wansink, in contrast, was retroactively creating hypotheses to fit data patterns that emerged after an experiment was over[1].In classic hypothesis testing, the hypothesis should always be made before the data is collected and analyzed. 
Hypothesis testing does not explain the reasons as to why does the difference exist. They simply indicate whether the difference is due to fluctuations due to some reason but the tests do not tell us as to which are the other reasons causing the difference.[2]

Another factor is data massaging that can manipulate the results. As stated in the article, a result is usually considered statistically significant when a calculation called a p-value is less than or equal to 0.05. But excessive data massaging can wind up with a p-value lower than 0.05 just by random chance, making a hypothesis seem valid when itâ€™s actually a fluke[1]. The tweaking of data to meet the usual arbitrary cutoff of 0.05 p-value and suiting your needs is something that is more than often done. 

Another flaw in hypothesis testing is that the result significance tests are based on probabilities and as such cannot be expressed with full certainty. Statistical inferences based on the significance tests cannot be said to be entirely correct evidences concerning the truth of the hypothesis. This is specially so in case of small samples where the probability of drawing erring inferences happens to be generally higher. For greater reliability, the size of samples be sufficiently enlarged.[2]

[1] https://www.buzzfeednews.com/article/stephaniemlee/brian-wansink-cornell-p-hacking
[2] https://www.wisdomjobs.com/e-university/research-methodology-tutorial-355/limitations-of-the-tests-of-hypotheses-11539.html#:~:text=Limitations%20of%20Hypothesis%20testing%20in%20Research&text=Important%20limitations%20are%20as%20follows,useful%20aids%20for%20decision%2Dmaking.

(e) The article talks about the prevalence of p-hacking and how it is really common amount scientists and researchers to tweak the results according to their needs to attain the p-value in a way that their results become statistically significant because the chances of data supported studies getting published becomes higher.

The article refers to the researchers who used text mining to calculate the extent of p-hacking. The study gave significant evidence the p-hacking is very widespread among the research community.Most researchers study for hypothesis and finding significant relationships with non zero effect sizes but it can not be avoided completely as academics to it for publications and career advancement.

Question 2
(a)
A Multiple Linear regression model is very useful in numerical continuous data but it can not be used to perform statistical analysis on this data set. The data set does not contain continuous variables but rather discrete variables which are nominal and we can not run regression model on nominal data since it can not be converted into ordinal data and the standard error terms will not be enough to provide a good model.

(b) The main regression equation in this part includes y = a + b1x1 + b2x2. As discussed earlier, since both variables x1 and x2 are discreet/categorical. The dependent variable y follows the binomial distribution as the outcome variables obtain a value of either 1 or 0. Hence, y~binomial(n,p) where n is the number of events and p is the probability of success. The link function is defined as g(p) = a + b1x1 + b2x2. As p is probability the output of g^-1 (inverse of link function) should lie between 0 and 1. By using a logit function, the equations show: logistic^-1(p) = logit(p) = ln(p/(1-p)) = a + b1x1 + b2x2.


(c)
```{r}
library(readr)
votedata <- read.csv("C:/Users/umers/OneDrive/Desktop/Waterloo/Courses/MSCI 718 Statistics/Assignment 2/voting_data.csv") #read data
str(votedata) # Checking the structure of voting data
logit_1 <- glm(dem_vote~ marital_id + state_id, family = binomial,data = votedata) #running the binomial logistic regression
logit_1
summary(logit_1) #summary of the regression
```
Running the binomial logistic regression, we get a coefficient value of -0.727 for the x1 variable i.e.marital_id which is also statistically significant since the p-value is less than 0.05. This means that married voters are more republican leaning and non married people are more likely to vote Democrats.
The B value is -0.727 so we calculate the odds ratio using e^B which is equal to 0.48. 1-e^B=0.52 which means that there is 52% chance of married people voting for the republicans.

##Question 3
part a and b
```{r}
myFunc = function(N,KW,KB,R,T) # Introducing function
{
library('gtools') #library for combination function
lenT=length(T) #length of the sequence T
usebag =c(rep('W',N), rep('B',N)) #an extra bag introduced to draw balls as per required
usebag
matr = matrix(NA,nrow = R+1, ncol = R) #introducing the matrix to find the possible number of combinations that can be drawn from bag A
y=R+1
x=R+1
for(i in 1:nrow(matr)) #for loop to add values to the matrix
{
  y=y-1
  x=y
  for(j in 1:ncol(matr))
  {
    if(x>0)
    {
      matr[i,j]='W'
      x=x-1
    }
    else
    {
      matr[i,j]='B'
    }
  }
}
matr #possible combinations of drawing balls from A to B
#matr = unique(combinations(n=length(usebag), r=R, v=usebag, set=FALSE, repeats.allowed=TRUE))
#matr
r1= nrow(matr) #number of rows
c1= ncol(matr) #number of columns
matB = matrix(NA,nrow =  r1, ncol = KW+KB) # introducing the matrix of balls in bag B
for(i in 1:nrow(matB)) #for loop to add values to the matrix of KW and KB.
{
    x=KW
    y=KB
  for(j in 1:ncol(matB))
  {
    if(x>0)
    {
      matB[i,j]='W'
      x=x-1
    }
    else if(y>0)
    {
      matB[i,j]='B'
      y=y-1
    }
  }
}
matB
newmatB =cbind(matB,matr,deparse.level = 1) #introducing the matrix to get the possible number of combinations that can be seen in bag B after drawing from bag A by combining matrix R and matrix B
newmatB
#Now we have all the possibilities for bag B compositions so we calculate maximum liklihood for each of them with the sequence of balls T
countmatB= matrix(0,nrow =  r1, ncol = 2) #newmatB is now converted into a count matrix of black and white balls with only 2 columns in it.
for(i in 1:nrow(newmatB))
{
   for(j in 1:ncol(newmatB))
   {
     if(newmatB[i,j]=='W')
     {
       countmatB[i,1]=countmatB[i,1]+1
     }
     else if(newmatB[i,j]=='B')
     {
       countmatB[i,2]=countmatB[i,2]+1
     }
   }
}
countmatB #possible outcomes in bag B
#maximum likelihood
mlh_vector = c() #introducing vector to store likelihood variables.
for(i in 1:nrow(countmatB)) #using for loop to check the likelihood of the sequence T for all possible combinations in bag B
{
  a=countmatB[i,1]
  b=countmatB[i,2]
  c=1
  for(j in 1:lenT)
  {
    if(T[j]=='W')
    {
      c=c*choose(a,1)
      a=a-1
    }
    else if(T[j]=='B')
    {
      c=c*choose(b,1)
      b=b-1
    }
    mlh_vector[i]=c
  }
}
mlh_vector #likelihood values stored in the vector
z=which.max(mlh_vector) #check index for maximum likelihood
best_draw =matr[z,] # check for the most likely draw on the same index in matrix R
best_draw
best_draw_n=c(0,0)
for(i in 1:R)
{
  if(best_draw[i] == 'W') 
     best_draw_n[1]=best_draw_n[1] +1
  else if(best_draw[i] == 'B')
     best_draw_n[2]= best_draw_n[2]+1
}
best_draw_n # count of black and white balls for the most likelihood
matA = matrix(NA,nrow = N+1, ncol = N) #introducing matrix A
y=N+1
x=N+1
for(i in 1:nrow(matA)) #for loop to store all possible combinations of black and white balls in bag A
{
  y=y-1
  x=y
  for(j in 1:ncol(matA))
  {
    if(x>0)
    {
      matA[i,j]='W'
      x=x-1
    }
    else
    {
      matA[i,j]='B'
    }
  }
}
matA
countmatA= matrix(0,nrow =  nrow(matA), ncol = 2) #bag A is now converted into a count matrix with only 2 columns in it.
for(i in 1:nrow(matA))
{
   for(j in 1:ncol(matA))
   {
     if(matA[i,j]=='W')
     {
       countmatA[i,1]=countmatA[i,1]+1
     }
     else if(matA[i,j]=='B')
     {
       countmatA[i,2]=countmatA[i,2]+1
     }
   }
}
countmatA
mlh_vector2 = c() #introducing vector to store likelihood variables for what draws are most likely with what combinations of balls in bag A
for(i in 1:nrow(countmatA)) #for loop to add likelihod values to the vector
{
  a=countmatA[i,1]
  b=countmatA[i,2]
  c=1
  c=c*choose(a,best_draw_n[1])*choose(b,best_draw_n[2])
  mlh_vector2[i]=c
}
mlh_vector2 #likelihood values stored in the vector
z=which.max(mlh_vector2) #check index for maximum likelihood.
best_drawA =matA[z,] # check for the most likely combination of A on the same index.
best_drawA
black_balls=0 #introducing variable of black balls
white_balls=0 #introducing variable of white balls
for(i in 1:N) #for loop to count black balls and white balls for the combination chosen
{
  if(best_drawA[i]=='W')
    white_balls=white_balls+1
  else if(best_drawA[i]=='B')
    black_balls=black_balls+1
}
white_balls
black_balls
result= matrix(0,nrow =  1, ncol = 2) #result stored in a matrix.
result[1,1]=white_balls
result[1,2]=black_balls
colnames(result) <- c("White_balls", "Black_balls")
result

}
myFunc(20,10,10,5,c('W','B','B','W','B','W')) #call the function for the values mentioned in part b.

```
